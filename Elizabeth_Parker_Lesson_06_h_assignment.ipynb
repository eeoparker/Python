{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 06 Assignment\n",
    "\n",
    "In this assignment, we want to read the `retail-churn.csv` dataset that we examined in a previous assignment and begin to pre-process it. The goal of the assignment is to become familiar with some common pre-processing and feature engineering steps by implementing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "col_names = ['user_id', 'gender', 'address', 'store_id', 'trans_id', 'timestamp', 'item_id', 'quantity', 'dollar']\n",
    "churn = pd.read_csv(\"../data/retail-churn.csv\", sep = \",\", skiprows = 1, names = col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic EDA\n",
    "present the first few rows and use pandas' `describe` to get an overview of the data\n",
    "<br/>&nbsp;&nbsp;<span style=\"color:red\" float:right>[0 point]</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id gender address  store_id  trans_id       timestamp       item_id  \\\n",
      "0   101981      F       E      2860    818463  11/1/2000 0:00  4.710000e+12   \n",
      "1   101981      F       E      2861    818464  11/1/2000 0:00  4.710000e+12   \n",
      "2   101981      F       E      2862    818465  11/1/2000 0:00  4.710000e+12   \n",
      "3   101981      F       E      2863    818466  11/1/2000 0:00  4.710000e+12   \n",
      "4   101981      F       E      2864    818467  11/1/2000 0:00  4.710000e+12   \n",
      "\n",
      "   quantity  dollar  \n",
      "0         1      37  \n",
      "1         1      17  \n",
      "2         1      23  \n",
      "3         1      41  \n",
      "4         8     288  \n",
      "            user_id       store_id      trans_id       item_id       quantity  \\\n",
      "count  2.522040e+05  252204.000000  2.522040e+05  2.522040e+05  252204.000000   \n",
      "mean   1.395660e+06  126101.500000  1.229771e+06  4.467833e+12       1.385692   \n",
      "std    6.094769e+05   72805.167983  2.350992e+05  1.679512e+12       3.705732   \n",
      "min    1.113000e+03       0.000000  8.177470e+05  2.000882e+07       1.000000   \n",
      "25%    9.937150e+05   63050.750000  1.025926e+06  4.710000e+12       1.000000   \n",
      "50%    1.586046e+06  126101.500000  1.233476e+06  4.710000e+12       1.000000   \n",
      "75%    1.862232e+06  189152.250000  1.433222e+06  4.710000e+12       1.000000   \n",
      "max    2.179605e+06  252203.000000  1.635482e+06  9.790000e+12    1200.000000   \n",
      "\n",
      "              dollar  \n",
      "count  252204.000000  \n",
      "mean      130.911389  \n",
      "std       388.142169  \n",
      "min         1.000000  \n",
      "25%        42.000000  \n",
      "50%        76.000000  \n",
      "75%       132.000000  \n",
      "max     70589.000000  \n"
     ]
    }
   ],
   "source": [
    "print(churn.head())\n",
    "print(churn.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new data frame will be called `churn_processed`, which stores the pre-processed columns as you run through each of the these steps. You will need to make sure your columns are properly named.\n",
    "\n",
    "1. Cast the `timestamp` column in churn into a column of type `datetime` and put the column into the `churn_processed` dataframe.  The new column should also be named `timestamp`.  Extract two new columns from `timestamp`: `dow` is the day of the week and `month` is the month of the year. Then drop the `timestamp` column from `churn_processed`.  Present the first few rows of `churn_processed`.\n",
    "<br/>&nbsp;&nbsp;<span style=\"color:red\" float:right>[2 point]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a new data frame by using the .DataFrame function. To cast timestamp to datetime I created a new column on the left side of the equation and did the datetime function on the right. To create the dow column its similar as above by creating the new columnm on the left side of the equation and the dayofweek property on the right. I had a hard time with this at first because I put () thinking dayofweek was a function and not a property. No () are neccessary. The same process was done for the month column. Then I dropped the timestamp column from the churn_processed dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dow  month\n",
      "0    2     11\n",
      "1    2     11\n",
      "2    2     11\n",
      "3    2     11\n",
      "4    2     11\n"
     ]
    }
   ],
   "source": [
    "# Add code here\n",
    "# Create the new (empty) data frame, called churn_processed\n",
    "churn_processed = pd.DataFrame() \n",
    "\n",
    "# Cast timestamp to datetime\n",
    "churn_processed['timestamp'] = pd.to_datetime(churn.timestamp)\n",
    "\n",
    "\n",
    "# Create a dow column\n",
    "churn_processed['dow'] = churn_processed['timestamp'].dt.dayofweek\n",
    "\n",
    "\n",
    "\n",
    "# Create a month column\n",
    "churn_processed['month'] = churn_processed['timestamp'].dt.month\n",
    "# Drop Timestamp\n",
    "churn_processed = churn_processed.drop(['timestamp'], axis =1) # have to assigned it back to the dataframe. I keep forgetting to do this!\n",
    "\n",
    "# See what we have\n",
    "print(churn_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Add `address` from `churn` to `churn_processed`. One-hot encode `address`, `dow` and `month`. Then drop columns `address`, `dow`, and `month` from `churn_processed`.  Finally, show some of the dataframe.\n",
    "<br/>&nbsp;&nbsp;<span style=\"color:red\" float:right>[2 point]</span>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created the new address column has I have done before. To do one-hot encoding I used the formula provided to us. I didn't need to drop the columns address, dow and month it does that already in the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dow  month address\n",
      "0    2     11       E\n",
      "1    2     11       E\n",
      "2    2     11       E\n",
      "3    2     11       E\n",
      "4    2     11       E\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dow_0</th>\n",
       "      <th>dow_1</th>\n",
       "      <th>dow_2</th>\n",
       "      <th>dow_3</th>\n",
       "      <th>dow_4</th>\n",
       "      <th>dow_5</th>\n",
       "      <th>dow_6</th>\n",
       "      <th>month_1</th>\n",
       "      <th>month_2</th>\n",
       "      <th>month_11</th>\n",
       "      <th>month_12</th>\n",
       "      <th>address_A</th>\n",
       "      <th>address_B</th>\n",
       "      <th>address_C</th>\n",
       "      <th>address_D</th>\n",
       "      <th>address_E</th>\n",
       "      <th>address_F</th>\n",
       "      <th>address_G</th>\n",
       "      <th>address_H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dow_0  dow_1  dow_2  dow_3  dow_4  dow_5  dow_6  month_1  month_2  \\\n",
       "0    0.0    0.0    1.0    0.0    0.0    0.0    0.0      0.0      0.0   \n",
       "1    0.0    0.0    1.0    0.0    0.0    0.0    0.0      0.0      0.0   \n",
       "2    0.0    0.0    1.0    0.0    0.0    0.0    0.0      0.0      0.0   \n",
       "3    0.0    0.0    1.0    0.0    0.0    0.0    0.0      0.0      0.0   \n",
       "4    0.0    0.0    1.0    0.0    0.0    0.0    0.0      0.0      0.0   \n",
       "\n",
       "   month_11  month_12  address_A  address_B  address_C  address_D  address_E  \\\n",
       "0       1.0       0.0        0.0        0.0        0.0        0.0        1.0   \n",
       "1       1.0       0.0        0.0        0.0        0.0        0.0        1.0   \n",
       "2       1.0       0.0        0.0        0.0        0.0        0.0        1.0   \n",
       "3       1.0       0.0        0.0        0.0        0.0        0.0        1.0   \n",
       "4       1.0       0.0        0.0        0.0        0.0        0.0        1.0   \n",
       "\n",
       "   address_F  address_G  address_H  \n",
       "0        0.0        0.0        0.0  \n",
       "1        0.0        0.0        0.0  \n",
       "2        0.0        0.0        0.0  \n",
       "3        0.0        0.0        0.0  \n",
       "4        0.0        0.0        0.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# add address column\n",
    "churn_processed['address'] = churn['address']\n",
    "\n",
    "print(churn_processed.head())\n",
    "# One-hot-encode address, dow and month\n",
    "\n",
    "# get package\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "onehot = OneHotEncoder(sparse = False) \n",
    "onehot.fit(churn_processed)\n",
    "col_names = onehot.get_feature_names_out(churn_processed.columns) # create column names\n",
    "churn_processed =  pd.DataFrame(onehot.transform(churn_processed), columns = col_names) # by assigning it to churn_processed we get rid of address, dow, and month\n",
    "\n",
    "# Show the dataframe\n",
    "churn_processed.head()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. So far we dropped `address`, `dow`, `month`, and `timestamp`.  Why would we want to drop all these columns?\n",
    "<br/>&nbsp;&nbsp;<span style=\"color:red\" float:right>[1 point]</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why would we want to drop all these columns?**<br/>\n",
    "Because they arn't in a format thats usuable for us. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Rescale `dollar` using min-max normalization. Use `pandas` and `numpy` to do it and call the rescaled column `dollar_std_minmax`.  Then see what the first few rows of the dataframe looks like.\n",
    "<br/>&nbsp;&nbsp;<span style=\"color:red\" float:right>[1 point]</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used min-max normalization and created a new column dollar_std_minmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id gender address  store_id  trans_id       timestamp  \\\n",
      "0        101981      F       E      2860    818463  11/1/2000 0:00   \n",
      "1        101981      F       E      2861    818464  11/1/2000 0:00   \n",
      "2        101981      F       E      2862    818465  11/1/2000 0:00   \n",
      "3        101981      F       E      2863    818466  11/1/2000 0:00   \n",
      "4        101981      F       E      2864    818467  11/1/2000 0:00   \n",
      "...         ...    ...     ...       ...       ...             ...   \n",
      "252199  2179605      B       G    251838   1630692  2/28/2001 0:00   \n",
      "252200  2179605      B       G    251839   1630821  2/28/2001 0:00   \n",
      "252201  2179605      B       G    251840   1630931  2/28/2001 0:00   \n",
      "252202  2179605      B       G    251841   1631033  2/28/2001 0:00   \n",
      "252203  2179605      B       G    251842   1631048  2/28/2001 0:00   \n",
      "\n",
      "             item_id  quantity  dollar  dolar_std_minmax  \n",
      "0       4.710000e+12         1      37               0.0  \n",
      "1       4.710000e+12         1      17               0.0  \n",
      "2       4.710000e+12         1      23               0.0  \n",
      "3       4.710000e+12         1      41               0.0  \n",
      "4       4.710000e+12         8     288               0.0  \n",
      "...              ...       ...     ...               ...  \n",
      "252199  2.250000e+12         2     138               0.0  \n",
      "252200  4.710000e+12         1      96               0.0  \n",
      "252201  4.710000e+12         1      89               0.0  \n",
      "252202  4.710000e+12         1     108               0.0  \n",
      "252203  4.710000e+12         1      95               0.0  \n",
      "\n",
      "[252204 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "# Min-max of dollar using numpy and pandas\n",
    "\n",
    "offset = min(churn.dollar) #important to remember to use churn.dollar and not just dollar. \n",
    "scale = max(churn.dollar) - min(churn.dollar)\n",
    "churn['dolar_std_minmax'] = (churn.dollar - offset)/scale\n",
    "# See what the dataframe looks like\n",
    "print(churn.round(decimals=2))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read about **robust normalization** [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html). The word **robust** in statistics generally refers to methods that behave reasonably even if the data is unusual.  For example, you can say that the median is a *robust* measure for the \"average\" of the data, while the mean is not.  In this respect a normalization is similar to an average. For a normalization **robust** might mean that the method is not affected by outliers. \n",
    "<br/><br/>\n",
    "5. Write briefly about what makes robust normalization different from Z-normalization.  Write briefly about what makes robust normalization more robust than Z-normalization.  Rescale `quantity` using robust normalization. Call the rescaled column `qty_std_robust` and add it to `churn_processed`.  Compare minimum, maximum, mean, standard deviation, and the median of the original churn['quantity'] with the robust-normalized churn_processed['qty_std_robust'].  Comment on what went wrong.\n",
    "<br/>&nbsp;&nbsp;<span style=\"color:red\" float:right>[3 point]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Robust normalization vs. Z-normalization**<br/>\n",
    "Z normalization puts the entire data set on scale where the mean of all values is 0 and the SD is 1. \n",
    "Robust normalization uses the forumla value = (value – median) / (p75 – p25) to normalize. \n",
    "\n",
    "**Robust normalization is more robust than Z-normalization**<br/>\n",
    "\n",
    "  By focusing on the 25% through 75% quartile you are getting rid of a lot of the noise in the data. Or in otherwords because outliers will appear outside the the 1st and 3rd quartile by using the 1st and 3rd you are minimizing the effects of either high or low outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the robust formula to do normalization for quantity but it proved to not be the a good fitting model which I explain why below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n",
      "        user_id gender address  store_id  trans_id       timestamp  \\\n",
      "0        101981      F       E      2860    818463  11/1/2000 0:00   \n",
      "1        101981      F       E      2861    818464  11/1/2000 0:00   \n",
      "2        101981      F       E      2862    818465  11/1/2000 0:00   \n",
      "3        101981      F       E      2863    818466  11/1/2000 0:00   \n",
      "4        101981      F       E      2864    818467  11/1/2000 0:00   \n",
      "...         ...    ...     ...       ...       ...             ...   \n",
      "252199  2179605      B       G    251838   1630692  2/28/2001 0:00   \n",
      "252200  2179605      B       G    251839   1630821  2/28/2001 0:00   \n",
      "252201  2179605      B       G    251840   1630931  2/28/2001 0:00   \n",
      "252202  2179605      B       G    251841   1631033  2/28/2001 0:00   \n",
      "252203  2179605      B       G    251842   1631048  2/28/2001 0:00   \n",
      "\n",
      "             item_id  quantity  dollar  dolar_std_minmax  qty_std_robust  \n",
      "0       4.710000e+12         1      37               0.0             NaN  \n",
      "1       4.710000e+12         1      17               0.0             NaN  \n",
      "2       4.710000e+12         1      23               0.0             NaN  \n",
      "3       4.710000e+12         1      41               0.0             NaN  \n",
      "4       4.710000e+12         8     288               0.0             inf  \n",
      "...              ...       ...     ...               ...             ...  \n",
      "252199  2.250000e+12         2     138               0.0             inf  \n",
      "252200  4.710000e+12         1      96               0.0             NaN  \n",
      "252201  4.710000e+12         1      89               0.0             NaN  \n",
      "252202  4.710000e+12         1     108               0.0             NaN  \n",
      "252203  4.710000e+12         1      95               0.0             NaN  \n",
      "\n",
      "[252204 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Compare minimum, maximum, mean, standard deviation, and the median of churn['quantity'] with churn_processed['qty_std_robust']\n",
    "offset = np.median(churn.quantity)\n",
    "scale = np.quantile(churn.quantity, 0.75) - np.quantile(churn.quantity, 0.25) # np.median(np.absolute(x - np.median(x)))\n",
    "print(scale, np.median(np.absolute(churn.quantity - np.median(churn.quantity))))\n",
    "churn['qty_std_robust'] = (churn.quantity - offset)/scale\n",
    "print(churn.round(decimals=2))\n",
    "\n",
    "\n",
    "# add address column\n",
    "churn_processed['qty_std_robust'] = churn['qty_std_robust']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    252204.000000\n",
      "mean          1.385692\n",
      "std           3.705732\n",
      "min           1.000000\n",
      "25%           1.000000\n",
      "50%           1.000000\n",
      "75%           1.000000\n",
      "max        1200.000000\n",
      "Name: quantity, dtype: float64\n",
      "count    54810.0\n",
      "mean         inf\n",
      "std          NaN\n",
      "min          inf\n",
      "25%          NaN\n",
      "50%          NaN\n",
      "75%          NaN\n",
      "max          inf\n",
      "Name: qty_std_robust, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/numpy/lib/function_base.py:3961: RuntimeWarning: invalid value encountered in subtract\n",
      "  diff_b_a = subtract(b, a)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(churn.quantity.describe())\n",
    "print(churn_processed.qty_std_robust.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Failure of Robust Normalization**<br/>\n",
    "the scale is zero and np.median(np.absolute(churn.quantity - np.median(churn.quantity is also zero. So you will end up trying to divide zero by zero. You can see the issue with quantity when you .describe it and see that the min, 25%, 50%, 75% are all 1. \n",
    "When you try to use robust normalization it wont work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Rescale `quantity` using Z-normalization, but normalize `quantity` **per user**, i.e. group by `user_id` so that the mean and standard deviation computed to normalize are computed separately by each `user_id`. Call the rescaled feature `qty_std_Z_byuser`. Present a histogram of `qty_std_Z_byuser`.  Briefly describe why and when you think this kind of normalization makes sense.\n",
    "<br/>&nbsp;&nbsp;<span style=\"color:red\" float:right>[3 point]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first I tried to use grouped without adding the .transform and realized that wouldn't work because grouped isn't a dataframe. After creating offset and scale you proceed as normal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        user_id gender address  store_id  trans_id       timestamp  \\\n",
      "0        101981      F       E      2860    818463  11/1/2000 0:00   \n",
      "1        101981      F       E      2861    818464  11/1/2000 0:00   \n",
      "2        101981      F       E      2862    818465  11/1/2000 0:00   \n",
      "3        101981      F       E      2863    818466  11/1/2000 0:00   \n",
      "4        101981      F       E      2864    818467  11/1/2000 0:00   \n",
      "...         ...    ...     ...       ...       ...             ...   \n",
      "252199  2179605      B       G    251838   1630692  2/28/2001 0:00   \n",
      "252200  2179605      B       G    251839   1630821  2/28/2001 0:00   \n",
      "252201  2179605      B       G    251840   1630931  2/28/2001 0:00   \n",
      "252202  2179605      B       G    251841   1631033  2/28/2001 0:00   \n",
      "252203  2179605      B       G    251842   1631048  2/28/2001 0:00   \n",
      "\n",
      "             item_id  quantity  dollar  dolar_std_minmax  qty_std_robust  \\\n",
      "0       4.710000e+12         1      37          0.000510             NaN   \n",
      "1       4.710000e+12         1      17          0.000227             NaN   \n",
      "2       4.710000e+12         1      23          0.000312             NaN   \n",
      "3       4.710000e+12         1      41          0.000567             NaN   \n",
      "4       4.710000e+12         8     288          0.004066             inf   \n",
      "...              ...       ...     ...               ...             ...   \n",
      "252199  2.250000e+12         2     138          0.001941             inf   \n",
      "252200  4.710000e+12         1      96          0.001346             NaN   \n",
      "252201  4.710000e+12         1      89          0.001247             NaN   \n",
      "252202  4.710000e+12         1     108          0.001516             NaN   \n",
      "252203  4.710000e+12         1      95          0.001332             NaN   \n",
      "\n",
      "        qty_std_Z_byuser  \n",
      "0                  -0.51  \n",
      "1                  -0.51  \n",
      "2                  -0.51  \n",
      "3                  -0.51  \n",
      "4                   6.95  \n",
      "...                  ...  \n",
      "252199              0.92  \n",
      "252200             -0.49  \n",
      "252201             -0.49  \n",
      "252202             -0.49  \n",
      "252203             -0.49  \n",
      "\n",
      "[252204 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 1: Group the data by 'user_id'\n",
    "grouped = churn.groupby('user_id')\n",
    "\n",
    "# Step 2: Calculate mean and standard deviation of 'quantity' for each group\n",
    "offset = grouped.quantity.transform('mean') # have to use .transform\n",
    "scale = grouped.quantity.transform('std')\n",
    "\n",
    "# Step 3: Z-normalize 'quantity' per user_id\n",
    "churn['qty_std_Z_byuser'] = (churn.quantity - offset) / scale\n",
    "\n",
    "# Round the values if desired\n",
    "churn['qty_std_Z_byuser'] = churn['qty_std_Z_byuser'].round(decimals=2)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(churn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wanted to center the graph around zero so I figured out how to do that using plt.xlim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Create a histogram\n",
    "plt.hist(churn['qty_std_Z_byuser'], bins=10, edgecolor='black')\n",
    "\n",
    "# Set the x-axis limits to center the histogram around zero\n",
    "plt.xlim(-10, 10)  \n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('qty_std_Z_byuser')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of qty_std_Z_byuser')\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What could be the purpose of this normalization?**\n",
    "<br/>\n",
    "Add comment here:\n",
    "This could be useful if you have a lot of variablity among the users or if the user data is on different scales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Convert `item_id` into a category column in `churn_processed`.  Replace the `item_id` of all the items sold only once in the entire data with `\"999999\"`.  How many item ids are of category `\"999999\"`?  Display 10 rows of `churn_processed` where `item_id` is category `\"999999\"`.\n",
    "<br/>&nbsp;&nbsp;<span style=\"color:red\" float:right>[2 point]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I created a new column in the churn_processed dataframe. then I used the astype(str) to turn item_id into a cateorical variable. Then I used .value_counts to create a list of how often each item_id occured. Then I created the variable itemWithOneTransaction which is what we are interested. Then using .loc and isin and the variables I previously made I assigned item_ids that only occured once to 999999. To get how many of 999999 there are I used the len function. To display 10 rows where item_id is 999999 I again used .loc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "876\n",
      "        dow_0  dow_1  dow_2  dow_3  dow_4  dow_5  dow_6  month_1  month_2  \\\n",
      "847       0.0    0.0    0.0    1.0    0.0    0.0    0.0      0.0      0.0   \n",
      "1415      0.0    0.0    0.0    1.0    0.0    0.0    0.0      0.0      0.0   \n",
      "1452      0.0    0.0    0.0    1.0    0.0    0.0    0.0      0.0      0.0   \n",
      "1457      0.0    0.0    0.0    1.0    0.0    0.0    0.0      0.0      0.0   \n",
      "1458      0.0    0.0    0.0    1.0    0.0    0.0    0.0      0.0      0.0   \n",
      "...       ...    ...    ...    ...    ...    ...    ...      ...      ...   \n",
      "249425    0.0    0.0    1.0    0.0    0.0    0.0    0.0      0.0      1.0   \n",
      "249854    0.0    0.0    1.0    0.0    0.0    0.0    0.0      0.0      1.0   \n",
      "250279    0.0    0.0    1.0    0.0    0.0    0.0    0.0      0.0      1.0   \n",
      "250377    0.0    0.0    1.0    0.0    0.0    0.0    0.0      0.0      1.0   \n",
      "250680    0.0    0.0    1.0    0.0    0.0    0.0    0.0      0.0      1.0   \n",
      "\n",
      "        month_11  ...  address_A  address_B  address_C  address_D  address_E  \\\n",
      "847          1.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
      "1415         1.0  ...        0.0        0.0        0.0        0.0        1.0   \n",
      "1452         1.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
      "1457         1.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
      "1458         1.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
      "...          ...  ...        ...        ...        ...        ...        ...   \n",
      "249425       0.0  ...        0.0        0.0        0.0        0.0        1.0   \n",
      "249854       0.0  ...        0.0        0.0        0.0        0.0        1.0   \n",
      "250279       0.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
      "250377       0.0  ...        0.0        0.0        0.0        0.0        0.0   \n",
      "250680       0.0  ...        0.0        0.0        0.0        0.0        1.0   \n",
      "\n",
      "        address_F  address_G  address_H  qty_std_robust  item_id  \n",
      "847           0.0        1.0        0.0             NaN   999999  \n",
      "1415          0.0        0.0        0.0             NaN   999999  \n",
      "1452          0.0        0.0        1.0             NaN   999999  \n",
      "1457          0.0        0.0        1.0             inf   999999  \n",
      "1458          0.0        0.0        1.0             inf   999999  \n",
      "...           ...        ...        ...             ...      ...  \n",
      "249425        0.0        0.0        0.0             inf   999999  \n",
      "249854        0.0        0.0        0.0             NaN   999999  \n",
      "250279        1.0        0.0        0.0             NaN   999999  \n",
      "250377        1.0        0.0        0.0             NaN   999999  \n",
      "250680        0.0        0.0        0.0             inf   999999  \n",
      "\n",
      "[876 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Added item_id column to churn_processed\n",
    "churn_processed['item_id'] = churn['item_id']\n",
    "\n",
    "# turned into a category column\n",
    "churn_processed['item_id'] = churn_processed['item_id'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "s = churn_processed['item_id'].value_counts()\n",
    "itemsWithOneTransaction = s.index[s == 1] # created variable here to make it easier to answer the question of how many ids are \"999999\" question. \n",
    "churn_processed.loc[churn_processed['item_id'].isin(itemsWithOneTransaction), 'item_id'] = '999999'\n",
    "print(len(itemsWithOneTransaction)) \n",
    "print(churn_processed.loc[churn_processed['item_id'] == '999999'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
