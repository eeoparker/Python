{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7662a8c6-16e6-483a-bda4-9a48f967e44e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assignment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f72dc24-839f-46f0-b59f-9ec61372ce61",
   "metadata": {
    "tags": []
   },
   "source": [
    "1. use pandas read_csv with sep='\\t' to read in the following 2 files available from the us naval academy:\n",
    "- url = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/keyword-tweets.txt'\n",
    "- url = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/general-tweets.txt'\n",
    "<br/> <span style=\"color:red\" float:right>[1 point]</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b08eaed-724c-47bd-8554-4c09dba1d9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cProfile\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.sparse import coo_matrix # this is the sparse matrix format discussed in lecture\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75ebf3f8-fd4a-4e1c-be12-ff1d2e9fb2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading corpus: Package 'corpus' not found in index\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "## A one-time requirement for these four downloads:\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('corpus')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca0dd414-1e46-49ef-948f-8e73e6d13a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/keyword-tweets.txt'\n",
    "tweet1 = pd.read_csv(url, sep = '\\t', header = None)\n",
    "\n",
    "url2 = 'https://www.usna.edu/Users/cs/nchamber/data/twitter/general-tweets.txt'\n",
    "tweet2 = pd.read_csv(url2, sep = '\\t', header = None)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dcfe90-2d80-483b-b9c5-fe658b3e078e",
   "metadata": {},
   "source": [
    "\n",
    "2. concatenate these 2 data sets into a single data frame called LabeledTweets that has 2 columns, named Sentiment and Tweet <span style=\"color:red\" float:right>[1 point]</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5dfa139-ee41-4f0e-852d-5f2ae0a46022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2004\n",
      "2000\n",
      "4004\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>Global Voices Online Â» Alex Castro: A liberal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>Do the Conservatives Have a Death Wish? http:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NOT</td>\n",
       "      <td>@MMFlint I've seen all of your movies and Capi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>RT @AllianceAlert: * House Dems ask for civili...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POLIT</td>\n",
       "      <td>RT @AdamSmithInst Quote of the week: My politi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                              Tweet\n",
       "0     POLIT  Global Voices Online Â» Alex Castro: A liberal...\n",
       "1     POLIT  Do the Conservatives Have a Death Wish? http:/...\n",
       "2       NOT  @MMFlint I've seen all of your movies and Capi...\n",
       "3     POLIT  RT @AllianceAlert: * House Dems ask for civili...\n",
       "4     POLIT  RT @AdamSmithInst Quote of the week: My politi..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LabeledTweets = tweet1.append(tweet2, ignore_index=True)\n",
    "print(len(tweet1)) # checked the length just to make sure it worked \n",
    "print(len(tweet2))\n",
    "print(len(LabeledTweets))\n",
    "\n",
    "LabeledTweets.columns =['Sentiment', 'Tweet']\n",
    "LabeledTweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773c84e1-4558-4cd0-a09c-b554fe4495e4",
   "metadata": {},
   "source": [
    "3. replace sentiment labels 'POLIT': 1, 'NOT': 0; <span style=\"color:red\" float:right>[0 point]</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7702b99a-0599-4045-a7a5-62d92b83b972",
   "metadata": {},
   "source": [
    "I used .mask to replace the old values of POLIT AND NOT to 1 and zero. I then used .astype to change Sentiment into an interger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a95c19c-e36e-4b70-9111-f89a99b61a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sentiment                                              Tweet\n",
      "0          1  Global Voices Online Â» Alex Castro: A liberal...\n",
      "1          1  Do the Conservatives Have a Death Wish? http:/...\n",
      "2          0  @MMFlint I've seen all of your movies and Capi...\n",
      "3          1  RT @AllianceAlert: * House Dems ask for civili...\n",
      "4          1  RT @AdamSmithInst Quote of the week: My politi...\n",
      "Sentiment     int64\n",
      "Tweet        object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "LabeledTweets.Sentiment = LabeledTweets.Sentiment.mask(LabeledTweets.Sentiment == \"POLIT\" , \"1\") \n",
    "LabeledTweets.Sentiment = LabeledTweets.Sentiment.mask(LabeledTweets.Sentiment == \"NOT\" , \"0\") \n",
    "LabeledTweets['Sentiment'] = LabeledTweets['Sentiment'].astype(int)\n",
    "print(LabeledTweets.head()) #Used this as a check\n",
    "print(LabeledTweets.dtypes) #Used this as a check "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d174792-6410-4dd3-a43c-e96d49b1fafe",
   "metadata": {},
   "source": [
    "\n",
    "4. clean the tweets\n",
    "   1. remove all tokens that contain a \"@\". Remove the whole token, not just the character.\n",
    "   2. remove all tokens that contain \"http\". Remove the whole token, not just the characters.\n",
    "   3. **replace** (not remove) all punctuation marks with a space (\" \")\n",
    "   4. **replace** all numbers with a space\n",
    "   5. **replace** all non ascii characters with a space\n",
    "   7. convert all characters to lowercase\n",
    "   8. strip extra whitespaces\n",
    "   9. lemmatize tokens\n",
    "   9. No need to remove stopwords because TfidfVectorizer will take care of that\n",
    "<br/><span style=\"color:red\" float:right>[9 point]</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69828eb1-fa27-4de8-9a08-a227ced52a63",
   "metadata": {},
   "source": [
    "Using the code given in class I changed it slightly to replace instead of remove where requested by the instructions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45f7fb25-97fb-471c-9a61-8a1fc7891183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Global Voices Online Â» Alex Castro: A liberal...</td>\n",
       "      <td>global voice online    alex castro a liberal l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Do the Conservatives Have a Death Wish? http:/...</td>\n",
       "      <td>do the conservative have a death wish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@MMFlint I've seen all of your movies and Capi...</td>\n",
       "      <td>i ve seen all of your movie and capitalism is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @AllianceAlert: * House Dems ask for civili...</td>\n",
       "      <td>rt house dems ask for civility at town hall an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>RT @AdamSmithInst Quote of the week: My politi...</td>\n",
       "      <td>rt quote of the week my political opinion lean...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                              Tweet  \\\n",
       "0          1  Global Voices Online Â» Alex Castro: A liberal...   \n",
       "1          1  Do the Conservatives Have a Death Wish? http:/...   \n",
       "2          0  @MMFlint I've seen all of your movies and Capi...   \n",
       "3          1  RT @AllianceAlert: * House Dems ask for civili...   \n",
       "4          1  RT @AdamSmithInst Quote of the week: My politi...   \n",
       "\n",
       "                                         clean_tweet  \n",
       "0  global voice online    alex castro a liberal l...  \n",
       "1              do the conservative have a death wish  \n",
       "2  i ve seen all of your movie and capitalism is ...  \n",
       "3  rt house dems ask for civility at town hall an...  \n",
       "4  rt quote of the week my political opinion lean...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(text, list_of_steps):\n",
    "    for step in list_of_steps:\n",
    "        if step == 'replace_non_ascii':\n",
    "            text = ''.join([i if ord(i) < 128 else ' ' for i in text]) # we want to replace anything <128 with a space. ' ' indicates space. \n",
    "        elif step == 'lowercase':\n",
    "            text = text.lower()\n",
    "        elif step == 'replace_punctuation':\n",
    "            text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "        elif step == 'replace_numbers':\n",
    "            text = re.sub(\"\\d+\", \" \", text)\n",
    "        elif step == 'strip_whitespace':\n",
    "            text = ' '.join(text.split())\n",
    "        elif step == 'remove_at':\n",
    "            word_list = text.split(' ')\n",
    "            word_list = [item for item in word_list if '@' not in item and 'http' not in item] # we only want to keep words in the list that dont have this condition and then we reconnect the list\n",
    "            text = ' '.join(word_list)\n",
    "        elif step == 'stem_words':\n",
    "            lmtzr = WordNetLemmatizer()\n",
    "            word_list = text.split(' ')\n",
    "            stemmed_words = [lmtzr.lemmatize(word) for word in word_list]\n",
    "            text = ' '.join(stemmed_words)\n",
    "    return text\n",
    "\n",
    "\n",
    "steps = ['remove_at', 'lowercase', 'replace_punctuation', 'replace_numbers', 'strip_whitespace', 'replace_non_ascii', 'stem_words'] #'replace_punctuation', 'replace_numbers', 'strip_whitespace',]\n",
    "#tweet_df['clean_tweet'] = tweet_df['tweet_text'].map(lambda s: preprocess(s, steps))\n",
    "#cProfile.run(\"LabeledTweets['clean_tweet'] = LabeledTweets['Tweet'].map(lambda s: preprocess(s, steps))\");\n",
    "LabeledTweets['clean_tweet'] = LabeledTweets['Tweet'].map(lambda s: preprocess(s, steps))\n",
    "display(LabeledTweets.head())\n",
    "#print(string.punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2667ec6e-144b-4502-83c4-03f987ab79f8",
   "metadata": {},
   "source": [
    "\n",
    "5. Use TfidfVectorizer from sklearn to prepare the data for machine learning.  Use max_features = 50;  <span style=\"color:red\" float:right>[2 point]</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e6f1f1-2e96-4b9a-979e-f981c06453df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4004x50 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6071 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, max_features = 50, stop_words = 'english')\n",
    "clean_texts = LabeledTweets['clean_tweet']\n",
    "tf_idf_tweets = vectorizer.fit_transform(clean_texts)\n",
    "\n",
    "tf_idf_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c75043c-495a-483f-9133-313d90a1a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(LabeledTweets)\n",
    "sentiment_list = LabeledTweets.Sentiment.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16522f97-ebfa-4b28-828d-77e336e29f3d",
   "metadata": {},
   "source": [
    "\n",
    "6. Use sklearn LogisticRegression to train a model on the  results on 75% of the data. <span style=\"color:red\" float:right>[1 point]</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ec785fd-4cbe-4b32-8dcb-a8ea5d8fe52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_targets = np.array([y for y in sentiment_list])\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_tweets, y_targets, test_size = 3003, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "698c6016-4460-449f-a937-463b18750916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87435b62-4835-4ae1-9d73-0ecc8e90c98b",
   "metadata": {},
   "source": [
    "\n",
    "7. determine the accuracy on the training data and the test data.   Determine the baseline accuracy. <span style=\"color:red\" float:right>[1 point]</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c396f757-d73a-4f8c-b7c8-f55588a4b401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8961038961038961\n",
      "Test accuracy: 0.8558108558108558\n",
      "Baseline accuracy: 0.5704295704295704\n"
     ]
    }
   ],
   "source": [
    "train_results = lr.predict(X_train)\n",
    "test_results = lr.predict(X_test)\n",
    "\n",
    "train_acc = np.mean(y_train == train_results)\n",
    "test_acc = np.mean(y_test == test_results)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))\n",
    "print('Baseline accuracy: {}'.format(np.max([np.mean(y_test == 1), np.mean(y_test == 0)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799767c5-8669-4916-a96c-5c394be75ec0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "8. Repeat steps 5, 6, and 7  with TfidfVectorizer max_features set to 5, 500, 5000, 50000 and discuss your accuracies. <span style=\"color:red\" float:right>[2 point]</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c3ab6a-37f6-47fa-a0b4-47fc6c561ca9",
   "metadata": {},
   "source": [
    "When the max_feature is too small it wont have enough to work with and the accuracy wont be that good. Generally as you increase the number the fit will get better, but at a certain point there will be diminishing returns where a bigger number doesn't neccessiarly make a better fitting model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22b4c24-1b94-4329-a456-7ab65c659b9d",
   "metadata": {},
   "source": [
    "Max_features = 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad56d152-b822-4ba0-a0d8-8921977d764d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7472527472527473\n",
      "Test accuracy: 0.7379287379287379\n",
      "Baseline accuracy: 0.5704295704295704\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, max_features = 5, stop_words = 'english')\n",
    "clean_texts = LabeledTweets['clean_tweet']\n",
    "tf_idf_tweets = vectorizer.fit_transform(clean_texts)\n",
    "\n",
    "tf_idf_tweets\n",
    "\n",
    "# type(LabeledTweets)\n",
    "sentiment_list = LabeledTweets.Sentiment.values.tolist()\n",
    "\n",
    "\n",
    "y_targets = np.array([y for y in sentiment_list])\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_tweets, y_targets, test_size = 3003, random_state = 42)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "train_results = lr.predict(X_train)\n",
    "test_results = lr.predict(X_test)\n",
    "\n",
    "train_acc = np.mean(y_train == train_results)\n",
    "test_acc = np.mean(y_test == test_results)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))\n",
    "print('Baseline accuracy: {}'.format(np.max([np.mean(y_test == 1), np.mean(y_test == 0)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a1eea74-aae2-4ffe-ba13-864b5d800e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9400599400599401\n",
      "Test accuracy: 0.8434898434898435\n",
      "Baseline accuracy: 0.5704295704295704\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, max_features = 500, stop_words = 'english')\n",
    "clean_texts = LabeledTweets['clean_tweet']\n",
    "tf_idf_tweets = vectorizer.fit_transform(clean_texts)\n",
    "\n",
    "tf_idf_tweets\n",
    "\n",
    "# type(LabeledTweets)\n",
    "sentiment_list = LabeledTweets.Sentiment.values.tolist()\n",
    "\n",
    "\n",
    "y_targets = np.array([y for y in sentiment_list])\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_tweets, y_targets, test_size = 3003, random_state = 42)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "train_results = lr.predict(X_train)\n",
    "test_results = lr.predict(X_test)\n",
    "\n",
    "train_acc = np.mean(y_train == train_results)\n",
    "test_acc = np.mean(y_test == test_results)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))\n",
    "print('Baseline accuracy: {}'.format(np.max([np.mean(y_test == 1), np.mean(y_test == 0)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97c0c92d-3b9e-403e-ab44-2ed58a5a509e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9690309690309691\n",
      "Test accuracy: 0.8081918081918081\n",
      "Baseline accuracy: 0.5704295704295704\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, max_features = 5000, stop_words = 'english')\n",
    "clean_texts = LabeledTweets['clean_tweet']\n",
    "tf_idf_tweets = vectorizer.fit_transform(clean_texts)\n",
    "\n",
    "tf_idf_tweets\n",
    "\n",
    "# type(LabeledTweets)\n",
    "sentiment_list = LabeledTweets.Sentiment.values.tolist()\n",
    "\n",
    "\n",
    "y_targets = np.array([y for y in sentiment_list])\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_tweets, y_targets, test_size = 3003, random_state = 42)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "train_results = lr.predict(X_train)\n",
    "test_results = lr.predict(X_test)\n",
    "\n",
    "train_acc = np.mean(y_train == train_results)\n",
    "test_acc = np.mean(y_test == test_results)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))\n",
    "print('Baseline accuracy: {}'.format(np.max([np.mean(y_test == 1), np.mean(y_test == 0)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03a44088-3390-4c5b-8d1b-50aee5e62005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.975024975024975\n",
      "Test accuracy: 0.7928737928737929\n",
      "Baseline accuracy: 0.5704295704295704\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(sublinear_tf = True, max_df = 0.5, max_features = 50000, stop_words = 'english')\n",
    "clean_texts = LabeledTweets['clean_tweet']\n",
    "tf_idf_tweets = vectorizer.fit_transform(clean_texts)\n",
    "\n",
    "tf_idf_tweets\n",
    "\n",
    "# type(LabeledTweets)\n",
    "sentiment_list = LabeledTweets.Sentiment.values.tolist()\n",
    "\n",
    "\n",
    "y_targets = np.array([y for y in sentiment_list])\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf_tweets, y_targets, test_size = 3003, random_state = 42)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "train_results = lr.predict(X_train)\n",
    "test_results = lr.predict(X_test)\n",
    "\n",
    "train_acc = np.mean(y_train == train_results)\n",
    "test_acc = np.mean(y_test == test_results)\n",
    "\n",
    "print('Train accuracy: {}'.format(train_acc))\n",
    "print('Test accuracy: {}'.format(test_acc))\n",
    "print('Baseline accuracy: {}'.format(np.max([np.mean(y_test == 1), np.mean(y_test == 0)])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69a2c9e-ccb2-4f01-9c74-92926dbb2f11",
   "metadata": {},
   "source": [
    "# End of assignment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
